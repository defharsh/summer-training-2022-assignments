{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FMK2312/Summer-Training-for-III-year/blob/main/Day1/Data%20Pre-processing/data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-qiINBQSK2g"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing libraries required to run code\n",
        "1) importing numpy as np. Numpy is used to manipulate data\n",
        "2) importing matplotlib for plotting\n",
        "3) importing pandas to load data and preprocess"
      ],
      "metadata": {
        "id": "BWL6ztmVOeYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h414sW4EOdJU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwEPNDWySTKm"
      },
      "source": [
        "dataset = pd.read_csv('sample_data/Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we have imported our data file 'Data.csv'\n",
        "We have assigned our Independent variable to X, which are first 3 columns of our data. [:,:-1] the first range is for selecting rows, as we have not provided any range it will select all the rows. Second range is for columns, here we have provided range :-1 which means we want all the column but not the last one.\n",
        "\n",
        "We have assigned our dependent variable to Y. we have only one dependent variable which is the last column."
      ],
      "metadata": {
        "id": "wd7Mfpw4O1NU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsz2yCebe1R",
        "outputId": "93c989bb-5a4d-40a9-f20c-10b2439bc5c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see our Independent Variables. Country, Age, Salary.\n",
        "These are Independent variable as there is no correlation and association between them. For ex, we cannot determine country from age and vice versa."
      ],
      "metadata": {
        "id": "G7fxNvr9VCT5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYrOQ43XcJR3",
        "outputId": "de0cc777-6aac-49e4-ba01-8fc2fb857787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y is our dependent variable. This is dependent as we calculated Y from X."
      ],
      "metadata": {
        "id": "o8sU327SVX8m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c93k7ipkSexq"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L4B7xEBgYPxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit learn or sklearn is an open source library. It features various classification, regression and clustering algorithms.\n",
        "Now, if we analyze our data, i.e X, we can see that at some places we have NaN i.e missing values in rows. Missing values need to be removed or handled as they can affect the performance of our model.\n",
        "Removing a row leads to loss of data, which leads to its own complications.\n",
        "What we can do is impute the missing values (infer them from existing values in columns). We will do this by using sklearn's impute module from which we will use SimpleImputer ftn.\n",
        "SimpleImputer is an Univariate feature imputer, which means we impute the missing values using the non missing values of that column itself where missing values are present.\n",
        "We will impute the columns 1 and 2.\n",
        "first we will fit our imputer with the data and then transform the columns using the imputer.\n",
        "Here we are using 'mean' as our strategy that is we are replacing our NaN values with mean of non missing values."
      ],
      "metadata": {
        "id": "Qu4wyAf_Wbds"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UgLdMS_bjq_",
        "outputId": "963b4a36-b794-41ad-da00-642da65cc613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see we have no NaN values. "
      ],
      "metadata": {
        "id": "RqvgO_k3YSYW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hwuVddlSwVi"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see in our data that we have categorical data present, that is, Country names.\n",
        "Models only work with numerical values. For this reason, it is necessary to convert the categorical values of the features into numerical ones, So the machine can learn from those data and gives the right model. This process of converting categorical data into numerical data is called Encoding.\n",
        "We are using ColumnTransformer from sklearn.compose to transform our categorical data column ie Country to numerical one.\n",
        "We will use OneHotEncoder() from sklearn.preprocessing to transform our column, we will pass this ftn as a param to set of params of ColumnTransfromer.\n",
        "Now lets take a look at Params of ColumnTransformer. 1) Transformers \n",
        "this specifies the type of transformation in this case, 'encoder', the ftn we using for this, 'OneHotEncoder()'. last one is Index of column we want to transform. \n",
        "2) remainder - this specifies what to do with the rest of columns. set to 'passthrough' we just concanate the colummns with the output of transformer. set to 'drop' we just drop all columns but the transformed one.\n"
      ],
      "metadata": {
        "id": "KfXKXuoCYd9w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7QspewyeBfx",
        "outputId": "7c59a4ae-f702-4225-b710-d706aef33cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHCShVyTOYY"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are using LabelEnoder to transform the categorical data (Yes/No) into 0/1."
      ],
      "metadata": {
        "id": "YKiqLOAGb8Wo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyhY8-gPpFCa",
        "outputId": "0963d438-e496-41ae-f67b-283e6481ff48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OneHotEncoding vs LabelEncoding\n",
        "\n",
        "\n",
        "LabelEncoding encodes the given column in the range of 0 - number of Classes-1. Now suppose we encoded our country column using LabelEncoder,\n",
        "we will assign each different country a number from 0 -> 2\n",
        "so for example france becomes 1, spain 0 etc. This is very straight forward right? it is. but it raises another problem. because we have labelencoded our independent categorical data, we have different numbers ranging from 0 -> 2 in the same column. bcz of this, the model will misunderstand the data to be in some kind of order, 0 < 1 < 2. But this isn’t the case at all. This sort of ranks the country, and our model will learn from this and give wrong prediction. So to avoid this, we never label encode our independent variables, only dependent variables.\n",
        "\n",
        "OneHotEncoding is done on independent variables. What one hot encoding does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns(basically we split a column into multiple depending on different categories present). The numbers are replaced by 1s and 0s, depending on which column has what value. In our example, we’ll get three new columns, one for each country — France, Germany, and Spain.\n",
        "\n",
        "For rows which have the first column value as France, the ‘France’ column will have a ‘1’ and the other two columns will have ‘0’s. Similarly, for rows which have the first column value as Germany, the ‘Germany’ column will have a ‘1’ and the other two columns will have ‘0’s.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LxRvBTfUcE_G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXgA6CzlqbCl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step is very straight forward. We are just splitting our data set into training and testing datasets, in the ratio of 8:2.\n",
        "The param random_state=1 in train_test_split splits the datasets into same data for every random_state set to 1."
      ],
      "metadata": {
        "id": "kk_vDApufJ_s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuwQhFdKrYTM",
        "outputId": "99273bf0-6ad5-4051-aee9-1d1c5bf46324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUrX_Tvcrbi4",
        "outputId": "88c5c9f2-6ccb-40f6-a68b-eb897388fd67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSMHiIsWreQY",
        "outputId": "14e77a05-c494-4d46-c8da-a8a81a1ce77c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_tW7H56rgtW",
        "outputId": "f86b5936-333a-4266-980e-4f63e07a3cd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxjSUXFQqo-3"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler()\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is the process of normalising the range of features in a dataset.\n",
        "Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. \n",
        "Feature scaling is of two types normalisation and standardization.\n",
        "\n",
        "\n",
        "Here we have used StandardScaler of sklearn, which is an example of standardization. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
        "\n",
        "MinMaxScaler on other hand is an example of normalisation. Normalization scales values between 0 and 1. This has application in neural networks where we recognize an object based on features. \n",
        "\n",
        "One important thing to notice here is, we have not fit our test dataset, we have just transformed it. \n",
        "\n",
        "This is a standard procedure when we scale our data. we use fit_transform() on our training data and transform() on our test data.\n",
        "\n",
        "If we will use the fit method on our test data too, we will compute a new mean and variance that is a new scale for each feature and will let our model learn about our test data too. Thus, what we want to keep as a surprise is no longer unknown to our model and we will not get a good estimate of how our model is performing on the test (unseen) data which is the ultimate goal of building a model using machine learning algorithm.\n",
        "\n",
        "fit_transform() is used on the training data so that we can scale the training data and also learn the scaling parameters of that data. Here, the model built by us will learn the mean and variance of the features of the training set. These learned parameters are then used to scale our test data. The fit method is calculating the mean and variance of each of the features present in our data. The transform method is transforming all the features using the respective mean and variance.\n",
        "\n",
        "Now, we want scaling to be applied to our test data too and at the same time do not want to be biased with our model. We want our test data to be a completely new and a surprise set for our model. The transform method helps us in this case.\n",
        "\n",
        "Using the transform method we can use the same mean and variance as it is calculated from our training data to transform our test data. Thus, the parameters learned by our model using the training data will help us to transform our test data.\n"
      ],
      "metadata": {
        "id": "E0tM4roCfw2u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWPET8ZdlMnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f346b93-8450-46b6-feed-86b0e9565cb0"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 0.5120772946859904 0.11428571428571432]\n",
            " [0.0 1.0 0.0 0.5652173913043479 0.45079365079365075]\n",
            " [1.0 0.0 0.0 0.7391304347826089 0.6857142857142855]\n",
            " [0.0 0.0 1.0 0.4782608695652175 0.37142857142857144]\n",
            " [0.0 0.0 1.0 0.0 0.0]\n",
            " [1.0 0.0 0.0 0.9130434782608696 0.8857142857142857]\n",
            " [0.0 1.0 0.0 1.0 1.0]\n",
            " [1.0 0.0 0.0 0.34782608695652173 0.2857142857142856]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXykB_QlRjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70d718e-c151-403a-8d6f-c4a57d7d2b1f"
      },
      "source": [
        "print(X_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 0.1304347826086958 0.17142857142857149]\n",
            " [1.0 0.0 0.0 0.43478260869565233 0.5428571428571427]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SeFfda3SN8d7"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}